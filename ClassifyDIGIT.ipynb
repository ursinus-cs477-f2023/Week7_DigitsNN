{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "001a0bbd",
   "metadata": {},
   "source": [
    "## Digit Classification with Nearest Neighbors\n",
    "\n",
    "### Chris Tralie\n",
    "\n",
    "Today we talked about <b>supervised learning</b>, which is the process by learning from a set of <b>labeled examples</b>, or examples that have been sorted into different classes beforehand.  We're not always fortunate enough to have data that's labeled like this, but when we do, there are a variety of techniques we can use to learn models of the different classes.  Actually, everything we've done in this class so far can be considered supervised learning, including learning from <a href = \"../../Assignments/HW3_Markov\">Markov chains trained on text</a> to <a href = \"../Week5_BagOfWords\">Naive Bayes bag of words</a> and <a href = \"../Week6_GradSchoolAdmissions\">Gaussian Naive Bayes</a>.  In every application we looked at with these techniques, we <b>trained</b> on our labeled examples and then we tested on some new unseen data that wasn't included in the training set.\n",
    "\n",
    "In this exercise today, we explore a new supervised learning technique known as <b>nearest neighbors</b>.  If we have a way of measuring a <b>distance</b> between two different data points, then we can apply this technique.  For example, let's suppose we had a labeled set of data points in two classes: red circles and blue squares.  Then, let's say we wanted to guess which of the two classes some new data point was in.  We'll depict this data point as a black triangle, as shown below\n",
    "\n",
    "<img src = \"NN2Dexample.svg\">\n",
    "\n",
    "\n",
    "The <b>K-nearest neighbors</b> technique simply finds the <b>K</b> closest labeled examples, as measured by the distance, and uses them to vote on the class identity of this new point.  In the above example, we choose <b>K = 5</b> for the 5 nearest neighbors, and we happen to get 4 votes for a red circle and 1 vote for a blue square, so we would label this new data point as a red circle.\n",
    "\n",
    "Overall, we can think of nearest neighbors as a supervised learning technique that <b>memorizes</b> examples.  This means it's only as good as the examples, and it will do better with a higher number and variety of examples, which we don't always have access to.  By contrast, other learning techniques will try to better <b>generalize</b> some knowledge to new examples.  But we'll start with this \"memorizer\" first.\n",
    "\n",
    "As simple as this technique may seem, it can work very well in practice.  Below we'll show k-nearest neighbors on an example of 28x28 images of drawn digits, where the labeled examples are obtained from the <a href = \"https://en.wikipedia.org/wiki/MNIST_database\">MNIST database</a>.  In this case, there are 10 unique classes for the digits between 0 and 9, inclusive. Let's first load in our imports and load in all of the MNIST digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aafcc09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage\n",
    "import skimage.io\n",
    "import scipy.stats\n",
    "from skimage.transform import resize\n",
    "from PIL import ImageTk, Image, ImageDraw\n",
    "import PIL\n",
    "from tkinter import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44aec08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in all digits\n",
    "res = 28\n",
    "digits = []\n",
    "for i in range(10):\n",
    "    digits.append([])\n",
    "    I = skimage.io.imread(\"Digits/{}.png\".format(i))/255.0\n",
    "    row = 0\n",
    "    col = 0\n",
    "    while row < I.shape[0]:\n",
    "        col = 0\n",
    "        while col < I.shape[1]:\n",
    "            img = I[row:row+res, col:col+res]\n",
    "            if np.sum(img) > 0:\n",
    "                digits[i].append(img)\n",
    "            col += res\n",
    "        row += res\n",
    "    print(len(digits[i]), \"unique \", i, \" digits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e356e0",
   "metadata": {},
   "source": [
    "The digits are setup in a 2D array so that <code>digits[i][j]</code> gives the $j^{\\text{th}}$ example of digit $i$.  Each digit is itself a $28 \\times 28$ 2D array of grayscale values between 0 and 1.\n",
    "\n",
    "Next, let's try to think about how to define a distance between two digit images.  First, let's look at the range of values in a digit.  We'll pick out the first 0 as an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce9bde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(digits[0][0], cmap='gray')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba627d75",
   "metadata": {},
   "source": [
    "In this case, we'll compute the <b>Euclidean distance</b> between digits, which is defined as \n",
    "\n",
    "### $\\sqrt{\\sum_{i, j} (X[i, j] - Y[i, j])^2}$\n",
    "\n",
    "and use this to find the top K nearest digits to a digit that you draw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f354892f",
   "metadata": {},
   "source": [
    "We'll setup a little interactive canvas where we can draw digits and retrieve their K nearest neighbors.  We'll use <code><a href = \"https://numpy.org/doc/stable/reference/generated/numpy.argsort.html\">np.argsort</a></code> to help us find the nearest neighbors.  Try it out for yourself!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc06348f",
   "metadata": {},
   "outputs": [],
   "source": [
    "width = 200  # canvas width\n",
    "height = 200 # canvas height\n",
    "center = height//2\n",
    "\n",
    "K = 10 # Number of nearest neighbors to consider\n",
    "\n",
    "def classify():\n",
    "    I = np.array(digit_image) # The image that we drew\n",
    "    I = I[:, :, 0]\n",
    "    dim = digits[0][0].shape[0]\n",
    "    I = resize(I, (dim, dim), anti_aliasing=True)\n",
    "    \n",
    "    res = 3\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plot_num = 2\n",
    "    \n",
    "    ########################################################\n",
    "    ## TODO: Find k nearest neighbors by looping through all digit examples\n",
    "    offset = 1\n",
    "    \n",
    "    dists = [] # Store a list of distances\n",
    "    idxs = [] # That's parallel with a list of tuples of (digit #, example index)\n",
    "    \n",
    "    guesses = [] # Fill in the array of guesses.  This should be the elements in idxs \n",
    "    # with the top K smallest distances\n",
    "    \n",
    "    \n",
    "    ########################################################\n",
    "\n",
    "    \n",
    "    ## Plot the guesses\n",
    "    for k, (num, idx) in enumerate(guesses):\n",
    "        digit = digits[num][idx]\n",
    "        plt.subplot(1, K+1, k+1)\n",
    "        J = np.zeros((I.shape[0], I.shape[1], 3))\n",
    "        J[:, :, 0] = I\n",
    "        J[:, :, 1] = digit\n",
    "        plt.imshow(J)\n",
    "        plt.title(\"{}\".format(num))\n",
    "        plt.axis(\"off\")\n",
    "        plot_num += 1\n",
    "    plt.subplot(1, K+1, offset)\n",
    "    plt.imshow(I, cmap='gray')\n",
    "    plt.axis(\"off\")\n",
    "        \n",
    "    root.destroy()\n",
    "\n",
    "def paint(event):\n",
    "    \"\"\"\n",
    "    Paint on the PIL canvas and the Tkinter canvas in parallel\n",
    "    Draw canvas will be saved, while Tkinter canvas shows\n",
    "    the user what they are drawing\n",
    "    \"\"\"\n",
    "    bs = 10\n",
    "    x1, y1 = (event.x - bs), (event.y - bs)\n",
    "    x2, y2 = (event.x + bs), (event.y + bs)\n",
    "    canvas.create_oval(x1, y1, x2, y2, fill=\"black\")\n",
    "    draw.ellipse([x1, y1, x2, y2], fill=\"#000000\")\n",
    "\n",
    "root = Tk()\n",
    "\n",
    "# create a tkinter canvas to draw on\n",
    "canvas = Canvas(root, width=width, height=height, bg='white')\n",
    "canvas.pack()\n",
    "\n",
    "# Create a PIL image and a drawer object\n",
    "digit_image = PIL.Image.new(\"RGB\", (width, height), (255, 255, 255))\n",
    "draw = ImageDraw.Draw(digit_image)\n",
    "canvas.pack(expand=YES, fill=BOTH)\n",
    "canvas.bind(\"<B1-Motion>\", paint)\n",
    "\n",
    "# add a button to save the image\n",
    "button=Button(text=\"classify\",command=classify)\n",
    "button.pack()\n",
    "\n",
    "root.mainloop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4e3b5f",
   "metadata": {},
   "source": [
    "It's worth noting that the above approach is a brute force nearest neighbors approach that uses sorting.  There are tons of ways to improve this.  One of them is to use a data structure known as a <a href = \"https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.KDTree.query.html#scipy.spatial.KDTree.query\">KD Tree</a>, which is able to hone in on the region of the space that contains nearest neighbors much more quickly without checking every example.  It is roughly analagous to <a href = \"https://ursinus-cs371-s2021.github.io/Modules/Module8/Exercise0\">binary search</a> performed spatially.  Sadly, KD Trees to suffer from what's known as the \"curse of dimensionality.\"  Therefore, one often uses an <a href = \"https://github.com/spotify/annoy\">approximate nearest neighbors</a> scheme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed002238",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
